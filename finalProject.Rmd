---
title: "Practical Machine Learning Project"
author: "KuangyuWang"
date: "04/20/2015"
output: html_document
---

# Data preprocessing

### Load libraries

```{r message=FALSE}
library(RCurl)
library(R.utils)
library(e1071)
library(caret)
library(glmnet)
```

### Download training and testing data files

```{r}
# Download training data
if(!file.exists("~/workspace/pml_proj/data/pml-training.csv")){
    download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "~/workspace/pml_proj/data/pml-training.csv", method="auto")
}
# Download testing data
if(!file.exists("~/workspace/pml_proj/data/pml-testing.csv")){
    download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "~/workspace/pml_proj/data/pml-testing.csv", method="auto")
}
```

### Read in data

```{r}
trainingDT = read.csv("~/workspace/pml_proj/data/pml-training.csv")
testingDT = read.csv("~/workspace/pml_proj/data/pml-testing.csv")
```

### Inspect features

*Handle numeric variables first.* Remove numeric variables with too much missing data or have *near zero variance*

```{r}
allNumericVars = colnames(trainingDT)[sapply(trainingDT, is.numeric)]
numeric2keepVars = allNumericVars
## Remove vars with more than 90% NAs
mostNA_vars = names(which(colMeans(is.na(trainingDT[,allNumericVars])) > 0.9))
numeric2keepVars = numeric2keepVars[! numeric2keepVars %in% c(mostNA_vars, "X", "raw_timestamp_part_1", "raw_timestamp_part_2")]
## Find and remove "near-zero variance" numeric vars
num_vars_variances = apply(trainingDT[,numeric2keepVars], 2, var, na.rm=T)
nearZeroVar_vars = names(which(num_vars_variances < 0.001))
## check distribution skewness
num_skewness = apply(trainingDT[,numeric2keepVars], 2, skewness, na.rm=T)
skewed_num_vars = numeric2keepVars[which(num_skewness > 2)]
## remove outliers
get_outlier_idx <- function(inputDT, queryVar){
  # find 99% value
  tmpQ99 = quantile(inputDT[[queryVar]], 0.99)
  # set cut-off
  tmpCutoff = 3 * tmpQ99
  # get and return outlier indices
  return(which(inputDT[[queryVar]] > tmpCutoff))
}
outlier_numeric_vars_idx_list = lapply(skewed_num_vars, function(x) get_outlier_idx(trainingDT, x))
outlier_numeric_vars_idx = unique(unlist(outlier_numeric_vars_idx_list))
# detected only one outlier, remove it now
trainingDT = trainingDT[-outlier_numeric_vars_idx,]
```

*Handle categorical variables next.* 

```{r}
# get those variables that R recognizes as non-numeric (some of them actually are numeric, need to fix them before proceeding)
other_vars = colnames(trainingDT)[!sapply(trainingDT, is.numeric)]
# identify those vars with more than 90% missing values
other_mostNA_vars = c()
for (i in other_vars) {
  if(length(which(trainingDT[,i] == "")) > nrow(trainingDT) * 0.9){
    other_mostNA_vars = c(other_mostNA_vars, i)
  }
}
# convert cvtd_timestamp to a new factor variable
trainingDT$cvtd_day = as.factor(as.Date(trainingDT$cvtd_timestamp))
testingDT$cvtd_day = as.factor(as.Date(testingDT$cvtd_timestamp))
# categorical variables to keep
other2keep_vars = c(other_vars[!other_vars %in% c(other_mostNA_vars,"classe","cvtd_timestamp")], "cvtd_day")
```


Center and scale all numeric variables for both training and testing data. Combine numeric and categorical variables to form new, transformed data set.

```{r}
#center and scale
preObj <- preProcess(trainingDT[,numeric2keepVars], method = c("center", "scale"))
trainTransformed <- predict(preObj, trainingDT[,numeric2keepVars])
testTransformed <- predict(preObj, testingDT[,numeric2keepVars])
#convert to matrix
trainTransformedMat = data.matrix(trainTransformed)
testTransformedMat = data.matrix(testTransformed)

nrow_train = nrow(trainTransformed)
nrow_test = nrow(testTransformed)
cat_var_DT = rbind(trainingDT[,other2keep_vars], testingDT[,other2keep_vars]) 
#create design matrix for categorical variables
cat_var_Mat = model.matrix(~ user_name + new_window + cvtd_day - 1, cat_var_DT)
cat_train_Mat = cat_var_Mat[1:nrow_train,]
cat_test_Mat = cat_var_Mat[(nrow_train+1):(nrow_train + nrow_test),]
# combine to form the final matrix 
trainTransformedMat = cbind(trainTransformedMat, cat_train_Mat)
testTransformedMat = cbind(testTransformedMat, cat_test_Mat)
```

# Model building

Bulid models using multiple regularized multinomial regression methods. For each method, conduct cross validation.

```{r}
# 10-fold Cross validation for each alpha = 0, 0.5, 1.0
fit.lasso.cv <- cv.glmnet(trainTransformedMat, trainingDT$classe, type.measure="class", alpha=1, 
                          family="multinomial", standardize=FALSE, nfolds=5)
#plot misclassification error VS. log(lambda)
plot(fit.lasso.cv, main="LASSO")

library(ROCR)
score = predict(fit.lasso.cv, newx = testTransformedMat, type = "response",s=fit.lasso.cv$lambda.min)
possibleClass = c("A","B","C","D","E")
predClass = possibleClass[max.col(score[,,1])] 
```

# Testing data prediction

Use prediction model to predict 20 different test cases and write files

```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(predClass)
```